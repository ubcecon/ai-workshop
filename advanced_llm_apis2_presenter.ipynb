{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 4.5 - Advanced - LLM APIs 2\n",
        "author: 'COMET Team <br> _Jonathan Graves, Alex Haddon_'\n",
        "date: 7 August 2024\n",
        "description: 'This notebook illustrates how to call different Large Language Models (LLMs) using their API, for the purposes of data analysis or computational use.'\n",
        "categories:\n",
        "  - advanced\n",
        "  - python\n",
        "  - OpenAI\n",
        "format:\n",
        "  html: default\n",
        "  ipynb:\n",
        "    jupyter:\n",
        "      kernelspec:\n",
        "        display_name: Python 3 (ipykernel)\n",
        "        language: python3\n",
        "        name: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are LLMs?\n",
        "\n",
        "Large Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the data they have been trained on. Examples of popular LLMs include GPT-3.5 from OpenAI and open-source models such as `ollama` or `huggingface`.\n",
        "\n",
        "## Applications of LLMs\n",
        "Large language models have a wide range of applications across various domains. In natural language understanding (NLU), they excel in tasks like text classification, named entity recognition, and language translation, enabling efficient content categorization and multilingual communication. LLMs are also powerful tools for text generation, facilitating the creation of articles, creative writing, and summarization of lengthy documents. Additionally, they enhance conversational agents and virtual assistants, providing human-like interactions and support. Furthermore, LLMs play a crucial role in knowledge extraction, sentiment analysis, and automated coding, making them invaluable in fields like customer support, market analysis, software development, and beyond. In fact, what you are reading right now was created using an LLM!\n",
        "\n",
        "Here is a [cool video](https://www.youtube.com/watch?v=5sLYAQS9sWQ&ab_channel=IBMTechnology) made by IBM that explains a little more about how LLMs work. \n",
        "\n",
        "\n",
        "# Setting Up the Environment\n",
        "\n",
        "Head to [ollama.com](https://ollama.com/) and download ollama locally. Then, in your terminal, run the code `ollama pull llama3` and wait for it to install\n",
        "\n",
        "## Installing Required Libraries\n",
        "Make sure to install the ollama library if you haven't already; in your terminal use the command `pip install ollama`. There will be various other packages you will be prompted to install later in this notebook. \n",
        "\n",
        "\n",
        "# Using an LLM (e.g., llama3)\n",
        "\n",
        "## Connecting to the LLM API\n",
        "Define a function to query the model by specifying the correct model as well as the prompt we want to pass to the model. \n",
        "\n",
        "NOTE: Make sure that you have the ollama application open and running locally before you try and make an API call or else you will get an error likely stating your connection has been \"refused\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ollama\n",
        "!pip install pandas\n",
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ollama\n",
        "from advanced_llm_apis2_tests import Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "response = ollama.chat(\n",
        "    model='llama3',  # specify the model \n",
        "    messages=[{'role': 'user', 'content': 'In fewer than 50 words, why is the sky blue?'}]) # insert the desired prompt\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of our API call to `ollama` comes in the [JSON](https://www.json.org/json-en.html) form which stands for JavaScript Object Notation. Essentially the output is split into a series of pairs consisting of a field name, colon, and then the value. For example, the output of our API call has `'model':'llama3'` as one of the entries meaning that the model we used to generate the response is llama3. If we want just the response to be the output we can specify that in our print statement using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only show the response from llama3\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you try! Fill in the code skeleton with the correct code. \n",
        "\n",
        "HINT: *In your prompt specify that you don't want a long response. Without that, ollama can take a very long time, especially if your machine is slower, as it is running locally rather than connecting to external servers.* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#response = ollama.chat(\n",
        "#     model= ...,\n",
        "#     messages=[{'role': 'user', 'content': ...}])\n",
        "\n",
        "# print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-Tests and Exercises\n",
        "\n",
        "## Multiple Choice Questions\n",
        "Here are a few questions you can use to check your understanding. Run the cell below before attempting the multiple choice questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1\n",
        "\n",
        "The output in JSON form uses the dictionary data type. What key (or sequence of keys) in the dictionary holds the output of the model?\n",
        "- A) ['model']\n",
        "- B) ['message']\n",
        "- C) ['message']['content']\n",
        "- D) ['content']\n",
        "- E) ['content']['message']\n",
        "- F) ['model']['message']\n",
        "\n",
        "*Enter your answer below as a a string with one of A,B,C,D,E,F ie. \"A\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer1 = #your answer here\n",
        "\n",
        "Tests.test1(answer1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2\n",
        "\n",
        "Out of the options below, which best describes what an LLM (Large Language Model) is?\n",
        "\n",
        "- A) A specialized algorithm for analyzing large datasets and generating insights.\n",
        "- B) A type of neural network that excels in generating human-like text based on extensive training data.\n",
        "- C) A tool designed for processing and translating spoken language into text.\n",
        "- D) A machine learning model primarily used for image and object recognition.\n",
        "\n",
        "*Enter your answer below as a a string with one of A,B,C,D ie. \"A\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer2 = #your answer here \n",
        "\n",
        "Tests.test2(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Worked Examples: Natural language processing and sentiment Analysis\n",
        "\n",
        "**Natural language processing** is a field of artificial intelligence that focuses on the interaction between computers and human languages. It aims to enable machines to understand, interpret, generate, and respond to human language in a way that is both meaningful and useful. One category of natural language processing is **sentiment analysis**: the NLP technique used to determine the emotional tone or sentiment expressed in a piece of text, also referred to as \"opinion mining\". It aims to classify text as positive, negative, neutral, or sometimes more granular emotional categories (e.g., anger, joy, sadness). Sentiment analysis is widely used for analyzing opinions, feedback, and emotions in social media posts, reviews, surveys, and other types of text data. \n",
        "\n",
        "Social scientists use sentiment analysis to assess the public’s reaction to government policies, decisions, and political figures, understand how public sentiment evolves during social movements, protests, or cultural shifts, and understand changes in collective behavior, as well as pick up on toxicity and radicalization in communities. For instance, Ederer et al. (2023) used LLM-powered sentiment analysis to classify posts by sentiment on the Econ Job Market Rumors (EJMR) online forum, finding that posts mentioning women or female pronouns (\"she\", \"her\", \"hers\") attracted significantly more hate speech, misogyny, and toxicity compared to posts directed at or mentioning men. \n",
        "\n",
        "## Problem Statement\n",
        "The issue with traditional sentiment analysis, also called lexicon-based sentiment analysis, is that it groups sentiment based on keywords pulled from a dictionary of pre-labelled words. For instance, the sentence: *\"While Biden did win the election, the five-point difference in the actual election results and the predicted polling shows that pollsters are extremely unreliable, and are most often shills for a particular political party.\"* Under traditional sentiment analysis, this sentence would be labelled as negative, as the words \"unreliable\" and \"shills\" are present in the negative words dictionary, and no words are present in the positive words dictionary. However, while efficient, this makes traditional analysis inaccurate in the modern digital landscape, where sentiment is often obfuscated by sarcasm, humor, and missing context. For instance, consider the following tweet:\n",
        "\n",
        "*\"YouTube now officially allow advertisers to display ads while videos are paused... actual piracy sites have less ads than youtube now.\"* (@7RlNGSINHAND, 2024)\n",
        "\n",
        "The sentiment here is clearly negative, expressing frustration at the plethora of ads on streaming sites. However, this is only obvious to the (human) reader, who understands the broader context of the tweet. The tweet itself contains no wording geared towards a particular sentiment, and would thus not be picked up as negative using traditional sentiment analysis methods. \n",
        "\n",
        "This is the issue that LLM-powered sentiment analysis attempts to solve: by *training* a LLM on large amounts of pre-labeled sentences, LLMs are able to pick up on the context clues, sarcasm, and humor that makes analyzing sentiment difficult. In this section, we introduce two methods of sentiment analysis. The first uses the ollama model we used earlier, and the second introduces BERT, another LLM, created specifically for sentiment analysis tasks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Installing Required Python Libraries\n",
        "\n",
        "We did this above when ran the command `pip install pandas` and `import pandas as pd` We will install pandas which will help us convert our survey responses into a machine readable data frame. Pandas is a popular Python library used for data manipulation and analysis. It provides powerful data structures like DataFrames and Series, which allow users to work with labeled and relational data intuitively. With pandas, you can easily read, clean, transform, and analyze data from various formats such as CSV, Excel, SQL databases, and more. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Read the CSV File\n",
        "\n",
        "First, we will read the `survey_responses_election.csv` file into a pandas DataFrame to load the survey responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file with the survey responses\n",
        "df = pd.read_csv('survey_responses_election.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the content\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Prepare the Responses for Sentiment Analysis\n",
        "\n",
        "Next, we’ll convert the survey responses from the DataFrame into a list format that can be passed to the LLM for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the 'Survey Response' column into a list\n",
        "responses = df['Survey Response'].tolist()\n",
        "\n",
        "# Print the list of responses to verify\n",
        "print(responses[:5])  # Display the first 5 responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Perform Sentiment Analysis Using an LLM\n",
        "\n",
        "Now that we have the responses in a list, we will use an LLM model (like llama3 from ollama) to perform sentiment analysis. The LLM will analyze the sentiments expressed in the survey responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a prompt for sentiment analysis\n",
        "response = ollama.chat(\n",
        "    model='llama3',  # specify the model\n",
        "    messages=[{'role': 'user', 'content': f\"Analyze the sentiment of the following survey responses:\\n{responses}\"}]\n",
        ")\n",
        "\n",
        "# Print the model's output (the sentiment analysis) formatted the output for better readability\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worked example #2: Sentiment analysis using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Set Up APIs\n",
        "\n",
        "Connect to Google Sheets and Google Drive API. For privacy reasongs, we have done this ahead of time but if you would like more information on how to do this please ask us after the presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Install Proper Libraries\n",
        "\n",
        "Install `gspread` and `oauth2client`. These libraries allow Python to access and interact with Google Sheets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: gspread in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (6.1.2)\n",
            "Collecting oauth2client\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from gspread) (2.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from gspread) (1.2.1)\n",
            "Collecting httplib2>=0.9.1 (from oauth2client)\n",
            "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from oauth2client) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in c:\\users\\irene\\appdata\\roaming\\python\\python312\\site-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from google-auth>=1.12.0->gspread) (5.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2>=0.9.1->oauth2client)\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\irene\\miniconda3\\envs\\idfk2\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.6.2)\n",
            "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
            "   ------------ --------------------------- 30.7/98.2 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 98.2/98.2 kB 2.8 MB/s eta 0:00:00\n",
            "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 96.9/96.9 kB ? eta 0:00:00\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "   ---------------------------------------- 0.0/104.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 104.1/104.1 kB ? eta 0:00:00\n",
            "Installing collected packages: pyparsing, httplib2, oauth2client\n",
            "Successfully installed httplib2-0.22.0 oauth2client-4.1.3 pyparsing-3.1.4\n"
          ]
        }
      ],
      "source": [
        "# Run this cell!\n",
        "!pip install gspread oauth2client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Authenticate and Access Google Sheets\n",
        "\n",
        "First, place the credentials file in your working directory: by moving the JSON credentials (from Google) file into your Jupyter notebook directory. Then, rename it to something easy to remember, such as credentials.json. Second, in your Jupyter notebook, use the following Python code to authenticate and access the responses from the Google Sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'credentials.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m scope \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://spreadsheets.google.com/feeds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Authenticate with the service account credentials\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m creds \u001b[38;5;241m=\u001b[39m \u001b[43mServiceAccountCredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json_keyfile_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredentials.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m client \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(creds)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Open the Google Sheet by name or URL\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\irene\\miniconda3\\envs\\idfk2\\Lib\\site-packages\\oauth2client\\service_account.py:219\u001b[0m, in \u001b[0;36mServiceAccountCredentials.from_json_keyfile_name\u001b[1;34m(cls, filename, scopes, token_uri, revoke_uri)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_json_keyfile_name\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    195\u001b[0m                            token_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, revoke_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Factory constructor from JSON keyfile by name.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m            the keyfile.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[0;32m    220\u001b[0m         client_credentials \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file_obj)\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_parsed_json_keyfile(client_credentials, scopes,\n\u001b[0;32m    222\u001b[0m                                          token_uri\u001b[38;5;241m=\u001b[39mtoken_uri,\n\u001b[0;32m    223\u001b[0m                                          revoke_uri\u001b[38;5;241m=\u001b[39mrevoke_uri)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'credentials.json'"
          ]
        }
      ],
      "source": [
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Define the scope for Google Sheets and Drive API\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# Authenticate with the service account credentials\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
        "client = gspread.authorize(creds)\n",
        "\n",
        "# Open the Google Sheet by name or URL\n",
        "sheet = client.open('LLM Sentiment Survey Responses').sheet1  # Replace with your sheet name\n",
        "\n",
        "# Fetch the responses from column 2 (assuming the first column is timestamp)\n",
        "responses = sheet.col_values(2)  # Column 2 contains the answers to \"How comfortable are you with LLMs?\"\n",
        "\n",
        "# Print the responses (optional)\n",
        "print(\"Survey Responses:\", responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'responses' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponses\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_responses.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'responses' is not defined"
          ]
        }
      ],
      "source": [
        "responses.to_csv(\"google_responses.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Perform Sentiment Analysis Using BERT\n",
        "\n",
        "Now that you have fetched the responses from the Google Sheet, you can analyze them using your chosen LLM. Here’s the Python code that sends the responses to the LLM for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "# Use the list of responses collected from the Google Sheet for sentiment analysis\n",
        "response = ollama.chat(\n",
        "    model='llama3',  # specify the model\n",
        "    messages=[{'role': 'user', 'content': f\"Analyze the sentiment of the following responses:\\n{responses}\"}]\n",
        ")\n",
        "\n",
        "# Print the model's output (the sentiment analysis)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
